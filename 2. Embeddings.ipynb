{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Векторные модели, которые мы рассматривали до этого, условно называются *счётными*. Они основываются на том, что так или иначе \"считают\" слова и их соседей, и на основе этого строят вектора для слов. \n",
    "\n",
    "Другой класс моделей, который более повсевмёстно распространён на сегодняшний день, называется *предсказательными* (или *нейронными*) моделями. Идея этих моделей заключается в использовании нейросетевых архитектур, которые \"предсказывают\" (а не считают) соседей слов. Одной из самых известных таких моделей является word2vec. Технология основана на нейронной сети, предсказывающей вероятность встретить слово в заданном контексте. Этот инструмент был разработан группой исследователей Google в 2013 году, руководителем проекта был Томаш Миколов (сейчас работает в Facebook).\n",
    "\n",
    "$$\\hat{P}(w_1^T) = \\prod_{t=1}^T \\hat{P}(w_t \\mid w_1^{t-1})$$\n",
    "\n",
    "Полученные таким образом вектора называются *распределенными представлениями слов*, или **эмбеддингами**.\n",
    "\n",
    "### Зачем это нужно?\n",
    "\n",
    "* Решать лингвистические задачи (в основном это про семантику и сочетаемость)\n",
    "* Подавать на вход нейронным сетям\n",
    "\n",
    "### Как это обучается?\n",
    "Мы задаём вектор для каждого слова с помощью матрицы $w$ и вектор контекста с помощью матрицы $W′$. По сути, word2vec является обобщающим названием для двух архитектур Skip-Gram и Continuous Bag-Of-Words (CBOW). \n",
    "\n",
    "**CBOW** предсказывает текущее слово, исходя из окружающего его контекста. \n",
    "\n",
    "**Skip-gram**, наоборот, использует текущее слово, чтобы предугадывать окружающие его слова. \n",
    "\n",
    "### Как это работает?\n",
    "Word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он создает словарь, «обучаясь» на входных текстовых данных, а затем вычисляет векторное представление слов. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл), в векторном представлении будут иметь близкие координаты векторов-слов. Для вычисления близости слов используется косинусное расстояние между их векторами.\n",
    "\n",
    "\n",
    "<img src=\"https://nycdatascience.com/blog/wp-content/uploads/2017/06/cossim.png\" width=\"500\">\n",
    "\n",
    "\n",
    "С помощью дистрибутивных векторных моделей можно строить семантические пропорции и решать примеры:\n",
    "\n",
    "* *король: мужчина = королева: женщина* \n",
    " $\\Rightarrow$ \n",
    "* *король - мужчина + женщина = королева*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы\n",
    "1. Матрицы слишком разреженные (→ очень большие)\n",
    "2. Невозможно установить тип семантических отношений между словами: и синонимы, и антонимы будут одинаково близки, т.к. обычно употребляются в схожих контекстах.\n",
    "\n",
    "\n",
    "## RusVectōrēs\n",
    "\n",
    "\n",
    "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятором семантической близости».\n",
    "\n",
    "<img src=\"./img/rusvectores2.png\" width=\"500\">\n",
    "\n",
    "Для других языков также можно найти предобученные модели — например, [модели fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/) (об этих моделях см. ниже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "Использовать предобученную модель w2v или обучить свою можно с помощью библиотеки `gensim`. В качестве обучающих данных возьмем размеченные и неразмеченные отзывы о фильмах (датасет взят с Kaggle). И те, и другие нужны, потому что чем больше данных, тем лучше будет векторная модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import nltk.data \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"./data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"./data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" \\\n",
    "      % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем из данных ссылки, html-разметку и небуквенные символы, а затем приводим все к нижнему регистру и токенизируем. На выходе получается массив из предложений, каждое из которых представляет собой массив слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, lang='english'):\n",
    "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(lang))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n",
      "Parsing sentences from unlabeled set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ancatmara/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "sentences = []  \n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set...\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем и сохраняем модель word2vec c параметрами, рекомендованными для этого датасета на Kaggle (размерность векторов = 300, минимальная частота слова = 40, размер контекстного окна = 10). Смотрим на время обучения модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "CPU times: user 5min 39s, sys: 2.05 s, total: 5min 41s\n",
      "Wall time: 1min 50s\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model = word2vec.Word2Vec(sentences, workers=4, size=300, min_count=40, window=10, sample=1e-3)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model_name = \"movie_reviews.model\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем оценить модель вручную, порешав примеры. Несколько дано ниже, попробуйте придумать свои."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actress', 0.7192949056625366)]\n",
      "[('men', 0.6240242719650269)]\n",
      "[('germany', 0.6882501840591431), ('china', 0.6826244592666626), ('europe', 0.6820869445800781)]\n",
      "novel\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
    "print(model.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
    "\n",
    "print(model.most_similar(\"usa\", topn=3))\n",
    "\n",
    "print(model.doesnt_match(\"comedy thriller western novel\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот пример загрузки готовой модели (это [скачанная с RusVectores](https://rusvectores.org/ru/models/) модель, обученная на НКРЯ в 2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"ruscorpora_mystem_cbow_300_2_2015.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "Скачайте [данные](https://www.dropbox.com/sh/aa9i9i6yf4tqygz/AABdDpec1N7V9_o9KUSMMpZ0a?dl=0) (корпус новостей на русском языке) и обучите на нем модель word2vec. Можете использовать как оригинальные тексты, так и лемматизированный корпус. \n",
    "\n",
    "1. Найдите по 5 ближайших слов к словам \"город\", \"спорт\", \"бизнес\", \"Россия\", \"происшествие\", \"река\", \"озеро\", \"море\", \"горы\", \"газпром\".\n",
    "2. Посчитайте семантическую близкость слов \"театр\" и \"кино\", \"Владивосток\" и \"Москва\", \"церковь\" и \"государство\", \"культура\" и \"отдых\", \"преступление\" и \"наказание\".\n",
    "3. Решите примеры:\n",
    "        * москва + екатеринбург - собянин\n",
    "        * спартак - москва + санкт-петербург\n",
    "        * иркутск - байкал + сочи\n",
    "        * татарстан - татарский + бурятия\n",
    "        * чай - лимон + кофе\n",
    "        * авиакомпания - аэрофлот + ржд\n",
    "4. Найдите лишнее, попробуйте интерпретировать результаты\n",
    "        * магазин, супермаркет, рынок, тц\n",
    "        * теннис, хоккей, футбол, дзюдо\n",
    "        * кошка, собака, попугай, кролик\n",
    "        * коми, дагестан, башкирия, камчатка\n",
    "\n",
    "Сохраните модель, она вам еще понадобится!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "word2vec с дополнительной меткой id документа, также реализован в `gensim`: `gensim.models.doc2vec`.\n",
    "\n",
    "\n",
    "![img](img/w2v_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грамов. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
    "\n",
    "https://github.com/facebookresearch/fasttext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE\n",
    "\n",
    "Модель, основанная на кодировании BPE (Byte Pair Encoding). Это один из способов представления текста, в котором мы используем в качестве токена не слова или символы, а наборы символов в зависимости от глубины кодирования.\n",
    "\n",
    "Скажем, мы хотим закодировать aaabdaaabac. В этой последовательности сочетание \"aa\" встречается наиболее часто. Мы можем \"слить\" эти буквы вместе и рассматривать их как отдельный символ. И так далее итеративно. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
